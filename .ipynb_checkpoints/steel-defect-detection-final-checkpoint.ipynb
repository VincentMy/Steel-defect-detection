{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLComp](https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master/pics/MLcomp.png)\n",
    "![Catalyst](https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master/pics/catalyst_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel demonstrates:\n",
    "\n",
    "1. Results of training models with [the training kernel](https://www.kaggle.com/lightforever/severstal-mlcomp-catalyst-train-0-90672-offline) and achieves 0.90672 score on public LB\n",
    "\n",
    "2. Useful code in MLComp library: TtaWrapp, ImageDataset, ChannelTranspose, rle utilities\n",
    "\n",
    "3. Output statistics and basic visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach descripton:\n",
    "\n",
    "1. Segmentation via 3 Unet networks. The predictions are being averaged. \n",
    "\n",
    "2. Thresholding and removeing small areas. This method gives 0.90672 on public LB.\n",
    "\n",
    "**Improving**:\n",
    "\n",
    "1. As many participations have seen, that is the key to remove false positives from your predictions.\n",
    "\n",
    "2. To cope with that, a classification network may be used. \n",
    "\n",
    "3. Heng CherKeng posted a classifier here: https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/106462#latest-634450 resent34_cls_01, **if you remove false positives with it you should get 0.9117 on LB**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the libraries:\n",
    "\n",
    "1. [MLComp](https://github.com/catalyst-team/mlcomp) is a distributed DAG  (Directed acyclic graph)  framework for machine learning with UI. It helps to train, manipulate, and visualize. All models in this kernel were trained offline via MLComp + Catalyst libraries. \n",
    "\n",
    "You can control an execution process via Web-site\n",
    "\n",
    "Dags\n",
    "![Dags](https://github.com/catalyst-team/mlcomp/blob/master/docs/imgs/dags.png?raw=true)\n",
    "\n",
    "Computers\n",
    "![Computers](https://github.com/catalyst-team/mlcomp/blob/master/docs/imgs/computers.png?raw=true)\n",
    "\n",
    "Reports\n",
    "![Reports](https://github.com/catalyst-team/mlcomp/blob/master/docs/imgs/reports.png?raw=true)\n",
    "\n",
    "Code\n",
    "![Code](https://github.com/catalyst-team/mlcomp/blob/master/docs/imgs/code.png?raw=true)\n",
    "\n",
    "Please follow [the web site](https://github.com/catalyst-team/mlcomp) to get the details.\n",
    "\n",
    "https://github.com/catalyst-team/mlcomp\n",
    "\n",
    "2. Catalys: High-level utils for PyTorch DL & RL research. It was developed with a focus on reproducibility, fast experimentation and code/ideas reusing. Being able to research/develop something new, rather then write another regular train loop. Break the cycle - use the Catalyst!\n",
    "\n",
    "https://github.com/catalyst-team/catalyst\n",
    "\n",
    "Docs and examples\n",
    "- Detailed [classification tutorial](https://github.com/catalyst-team/catalyst/blob/master/examples/notebooks/classification-tutorial.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/catalyst-team/catalyst/blob/master/examples/notebooks/classification-tutorial.ipynb)\n",
    "- Comprehensive [classification pipeline](https://github.com/catalyst-team/classification).\n",
    "\n",
    "API documentation and an overview of the library can be found here\n",
    "[![Docs](https://img.shields.io/badge/dynamic/json.svg?label=docs&url=https%3A%2F%2Fpypi.org%2Fpypi%2Fcatalyst%2Fjson&query=%24.info.version&colorB=brightgreen&prefix=v)](https://catalyst-team.github.io/catalyst/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128x128.pth\t       unet_mobilenet2.pth  unet_se_resnext50_32x4d.pth\r\n",
      "resnet34_classify.pth  unet_resnet34.pth\r\n"
     ]
    }
   ],
   "source": [
    "! ls ../input/severstalmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install MLComp library(offline version):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the competition does not allow commit with the kernel that uses internet connection, we use offline installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished = 1 failed = 0\r\n",
      "finished = 2 failed = 0\r\n",
      "finished = 3 failed = 0\r\n",
      "finished = 7 failed = 0\r\n",
      "finished = 9 failed = 0\r\n",
      "finished = 10 failed = 0\r\n",
      "finished = 11 failed = 0\r\n",
      "finished = 12 failed = 0\r\n",
      "finished = 16 failed = 0\r\n",
      "finished = 18 failed = 0\r\n",
      "finished = 22 failed = 0\r\n",
      "finished = 24 failed = 0\r\n",
      "finished = 26 failed = 0\r\n",
      "INSTALLATION SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "! python ../input/mlcomp/mlcomp/mlcomp/setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.jit import load\n",
    "\n",
    "from mlcomp.contrib.transform.albumentations import ChannelTranspose\n",
    "from mlcomp.contrib.dataset.classify import ImageDataset\n",
    "from mlcomp.contrib.transform.rle import rle2mask, mask2rle\n",
    "from mlcomp.contrib.transform.tta import TtaWrap\n",
    "\n",
    "#import os\n",
    "import sys\n",
    "import pdb\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "#import torch\n",
    "#print(torch.__version__)\n",
    "#import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "from torch.nn import functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "import albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catalyst allows to trace models. That is an extremely useful features in Pytorch since 1.0 version: \n",
    "\n",
    "https://pytorch.org/docs/stable/jit.html\n",
    "\n",
    "Now we can load models without re-defining them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_se_resnext50_32x4d = \\\n",
    "    load('/kaggle/input/severstalmodels/unet_se_resnext50_32x4d.pth').cuda()\n",
    "unet_mobilenet2 = load('/kaggle/input/severstalmodels/unet_mobilenet2.pth').cuda()\n",
    "unet_resnet34 = load('/kaggle/input/severstalmodels/unet_resnet34.pth').cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models' mean aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        res = []\n",
    "        x = x.cuda()\n",
    "        with torch.no_grad():\n",
    "            for m in self.models:\n",
    "                res.append(m(x))\n",
    "        res = torch.stack(res)\n",
    "        return torch.mean(res, dim=0)\n",
    "\n",
    "model = Model([unet_se_resnext50_32x4d, unet_mobilenet2, unet_resnet34])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TTA transforms, datasets, loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def create_transforms(additional):\n",
    "    res = list(additional)\n",
    "    # add necessary transformations\n",
    "    res.extend([\n",
    "        A.Normalize(\n",
    "            mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n",
    "        ),\n",
    "        ChannelTranspose()\n",
    "    ])\n",
    "    res = A.Compose(res)\n",
    "    return res\n",
    "\n",
    "img_folder = '/kaggle/input/severstal-steel-defect-detection/test_images'\n",
    "batch_size = 2\n",
    "num_workers = 0\n",
    "\n",
    "# Different transforms for TTA wrapper\n",
    "transforms = [\n",
    "    [],\n",
    "    [A.HorizontalFlip(p=1)]\n",
    "]\n",
    "\n",
    "transforms = [create_transforms(t) for t in transforms]\n",
    "datasets = [TtaWrap(ImageDataset(img_folder=img_folder, transforms=t), tfms=t) for t in transforms]\n",
    "loaders = [DataLoader(d, num_workers=num_workers, batch_size=batch_size, shuffle=False) for d in datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loaders' mean aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e6ae23c37a4609b09de9a174995260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=900), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "thresholds = [0.5, 0.5, 0.5, 0.5]\n",
    "min_area = [600, 600, 1000, 2000]\n",
    "\n",
    "res = []\n",
    "# Iterate over all TTA loaders\n",
    "total = len(datasets[0])//batch_size\n",
    "for loaders_batch in tqdm_notebook(zip(*loaders), total=total):\n",
    "    preds = []\n",
    "    image_file = []\n",
    "    for i, batch in enumerate(loaders_batch):\n",
    "        features = batch['features'].cuda()\n",
    "        p = torch.sigmoid(model(features))\n",
    "        # inverse operations for TTA\n",
    "        p = datasets[i].inverse(p)\n",
    "        preds.append(p)\n",
    "        image_file = batch['image_file']\n",
    "    \n",
    "    # TTA mean\n",
    "    preds = torch.stack(preds)\n",
    "    preds = torch.mean(preds, dim=0)\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    \n",
    "    # Batch post processing\n",
    "    for p, file in zip(preds, image_file):\n",
    "        file = os.path.basename(file)\n",
    "        # Image postprocessing\n",
    "        for i in range(4):\n",
    "            p_channel = p[i]\n",
    "            imageid_classid = file+'_'+str(i+1)\n",
    "            p_channel = (p_channel>thresholds[i]).astype(np.uint8)\n",
    "            if p_channel.sum() < min_area[i]:\n",
    "                p_channel = np.zeros(p_channel.shape, dtype=p_channel.dtype)\n",
    "\n",
    "            res.append({\n",
    "                'ImageId_ClassId': imageid_classid,\n",
    "                'EncodedPixels': mask2rle(p_channel)\n",
    "            })\n",
    "        \n",
    "df_seg = pd.DataFrame(res)\n",
    "#df.to_csv('submission.csv', index=False)\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B0 CLS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 2. Buliding the CLS model '''\n",
    "import sys\n",
    "pack_path = '/kaggle/input/efficientnet-pytorch/efficientnet-pytorch/EfficientNet-PyTorch-master'\n",
    "sys.path.append(pack_path)\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteelDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        \n",
    "        df['ImageId']  = df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\n",
    "        self.image_ids = df['ImageId'].unique().tolist()\n",
    "        \n",
    "        self.transform = albumentations.Compose(\n",
    "                                  [\n",
    "                                    albumentations.HorizontalFlip(p=0.5),\n",
    "                                    albumentations.VerticalFlip(p=0.5),\n",
    "                                    albumentations.Resize(150,938),\n",
    "                                    albumentations.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), p=1)\n",
    "                                  ]\n",
    "                                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image_id = self.image_ids[index]\n",
    "        path = os.path.join('/kaggle/input/severstal-steel-defect-detection/test_images', image_id)\n",
    "        image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "        augment = self.transform(image=image)\n",
    "        image = augment['image'].transpose(2, 0, 1)\n",
    "        image = torch.from_numpy(image)\n",
    "        \n",
    "        return image_id, image\n",
    "\n",
    "''' 4. Dataloader '''\n",
    "df = pd.read_csv('/kaggle/input/severstal-steel-defect-detection/sample_submission.csv')\n",
    "\n",
    "test_dataset = SteelDataset(df)\n",
    "\n",
    "test_loader  = DataLoader(\n",
    "                            test_dataset,\n",
    "                            batch_size  = 8,\n",
    "                            shuffle     = False,\n",
    "                            num_workers = 4,\n",
    "                            pin_memory  = True,\n",
    "                         )\n",
    "\n",
    "''' 5. Prediction '''\n",
    "\n",
    "def sharpen(p,t=0.5):\n",
    "        if t!=0:\n",
    "            return p**t\n",
    "        else:\n",
    "            return p\n",
    "\n",
    "#check_point = torch.load('/kaggle/input/150938last2/B0_150938_01347.pth')\n",
    "check_point2 = torch.load('/kaggle/input/1601000last/B0_1601000_314_epoch27.pth')\n",
    "\n",
    "#cls_model = EfficientNet.from_name('efficientnet-b0')\n",
    "cls_model2 = EfficientNet.from_name('efficientnet-b0')\n",
    "\n",
    "#in_features = cls_model._fc.in_features\n",
    "in_features2 = cls_model2._fc.in_features\n",
    "\n",
    "#cls_model._fc = nn.Linear(in_features, 4)\n",
    "cls_model2._fc = nn.Linear(in_features2, 4)\n",
    "\n",
    "\n",
    "#cls_model.load_state_dict(check_point['state_dict'], strict=True)\n",
    "cls_model2.load_state_dict(check_point2['state_dict'], strict=True)\n",
    "\n",
    "def get_preds(model2, dataloader):\n",
    "    test_probability_label = []\n",
    "    test_id = []\n",
    "    \n",
    "    for t, (image_ids, images) in enumerate(dataloader):\n",
    "        batch_size, C, H, W = images.shape\n",
    "        images = images.cuda()\n",
    "        \n",
    "        #model = model.cuda()\n",
    "        model2 = model2.cuda()\n",
    "        with torch.no_grad():\n",
    "            #model.eval()\n",
    "            model2.eval()\n",
    "            \n",
    "            num_augment = 0\n",
    "            if 1:\n",
    "                #logit = model(images)\n",
    "                logit2 = model2(images)\n",
    "                #probability = torch.sigmoid(logit)\n",
    "                probability2 = torch.sigmoid(logit2)\n",
    "                probability_label = sharpen(probability2, 0)\n",
    "                #probability_label += sharpen(probability2, 0)\n",
    "                \n",
    "                num_augment += 1\n",
    "                \n",
    "            if 'flip_lr' in augment:    # 对一组图像的每一张都进行最后一个维度的翻转,即左右翻转. shape=[bs, C, H, W]\n",
    "                #logit = model(torch.flip(images, dims=[3]))\n",
    "                logit2 = model2(torch.flip(images, dims=[3]))\n",
    "                #probability = torch.sigmoid(logit)\n",
    "                probability2 = torch.sigmoid(logit2)\n",
    "                \n",
    "                #probability_label += sharpen(probability, 0)\n",
    "                probability_label += sharpen(probability2, 0)\n",
    "                num_augment += 1\n",
    "            \n",
    "            if 'flip_ud' in augment:    # 对一组图像的每一张都进行倒数第二个维度的翻转,即上下翻转. shape=[bs, C, H, W]\n",
    "                #logit = model(torch.flip(images, dims=[2]))\n",
    "                logit2 = model2(torch.flip(images, dims=[2]))\n",
    "                #probability = torch.sigmoid(logit)\n",
    "                probability2 = torch.sigmoid(logit2)\n",
    "                \n",
    "                #probability_label += sharpen(probability, 0)\n",
    "                probability_label += sharpen(probability2, 0)\n",
    "                num_augment += 1\n",
    "                \n",
    "            probability_label = probability_label / num_augment\n",
    "        \n",
    "        probability_label = probability_label.data.cpu().numpy()\n",
    "        \n",
    "        test_probability_label.append(probability_label)\n",
    "        test_id.extend([i for i in image_ids])\n",
    "        \n",
    "    test_probability_label = np.concatenate(test_probability_label)\n",
    "    \n",
    "    return test_probability_label, test_id\n",
    "\n",
    "threshold = [0.5, 0.5, 0.5, 0.5]\n",
    "augment = ['null', 'flip_lr','flip_ud']\n",
    "\n",
    "probability_label, image_id = get_preds(cls_model2, test_loader)\n",
    "pred = np.digitize(probability_label, threshold)\n",
    "\n",
    "image_id_class_id = []\n",
    "encoded_pixel = []\n",
    "for b in range(len(image_id)):\n",
    "    for c in range(4):\n",
    "        image_id_class_id.append(image_id[b]+'_%d'%(c+1))\n",
    "        if pred[b, c] == 0:\n",
    "            rle = ''\n",
    "        else:\n",
    "            rle = '1 1'\n",
    "        encoded_pixel.append(rle)\n",
    "\n",
    "df_classification = pd.DataFrame(zip(image_id_class_id, encoded_pixel), columns=['ImageId_ClassId', 'EncodedPixels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n"
     ]
    }
   ],
   "source": [
    "#df = pd.DataFrame(res)\n",
    "#df = df.fillna('')\n",
    "assert(np.all(df_seg['ImageId_ClassId'].values == df_classification['ImageId_ClassId'].values))\n",
    "print((df_seg.loc[df_classification['EncodedPixels'] == '','EncodedPixels'] != '').sum())\n",
    "df_seg.loc[df_classification['EncodedPixels']=='','EncodedPixels']=''\n",
    "df_seg.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_seg['Image'] = df_seg['ImageId_ClassId'].map(lambda x: x.split('_')[0])\n",
    "#df_seg['Class'] = df_seg['ImageId_ClassId'].map(lambda x: x.split('_')[1])\n",
    "#df_seg['empty'] = df_seg['EncodedPixels'].map(lambda x: not x)\n",
    "#df_seg[def_seg['empty'] == False]['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n%matplotlib inline\\n\\ndf_seg = pd.read_csv('submission.csv')[:40]\\ndf_seg['Image'] = df_seg['ImageId_ClassId'].map(lambda x: x.split('_')[0])\\ndf_seg['Class'] = df_seg['ImageId_ClassId'].map(lambda x: x.split('_')[1])\\n\\nfor row in df.itertuples():\\n    img_path = os.path.join(img_folder, row.Image)\\n    img = cv2.imread(img_path)\\n    mask = rle2mask(row.EncodedPixels, (1600, 256))         if isinstance(row.EncodedPixels, str) else np.zeros((256, 1600))\\n    if mask.sum() == 0:\\n        continue\\n    \\n    fig, axes = plt.subplots(1, 2, figsize=(20, 60))\\n    axes[0].imshow(img/255)\\n    axes[1].imshow(mask*60)\\n    axes[0].set_title(row.Image)\\n    axes[1].set_title(row.Class)\\n    plt.show()\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "%matplotlib inline\n",
    "\n",
    "df_seg = pd.read_csv('submission.csv')[:40]\n",
    "df_seg['Image'] = df_seg['ImageId_ClassId'].map(lambda x: x.split('_')[0])\n",
    "df_seg['Class'] = df_seg['ImageId_ClassId'].map(lambda x: x.split('_')[1])\n",
    "\n",
    "for row in df.itertuples():\n",
    "    img_path = os.path.join(img_folder, row.Image)\n",
    "    img = cv2.imread(img_path)\n",
    "    mask = rle2mask(row.EncodedPixels, (1600, 256)) \\\n",
    "        if isinstance(row.EncodedPixels, str) else np.zeros((256, 1600))\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 60))\n",
    "    axes[0].imshow(img/255)\n",
    "    axes[1].imshow(mask*60)\n",
    "    axes[0].set_title(row.Image)\n",
    "    axes[1].set_title(row.Class)\n",
    "    plt.show()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "08bdf88461e542f1a51c8ee6f921fa3c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "19dbf19af5004839857f4a52fa273c64": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "282e42d5b07448b7a6f6433a5305421a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "55e5f329c5a94f019e296c19ed5a93bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9f0f1b898ae94430a4932fe5e28d606b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b4da74adf29e4d89be098ff0a9aa26bf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_282e42d5b07448b7a6f6433a5305421a",
       "max": 900,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_08bdf88461e542f1a51c8ee6f921fa3c",
       "value": 900
      }
     },
     "c144991e20514a4abf9612e81a97645f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_19dbf19af5004839857f4a52fa273c64",
       "placeholder": "​",
       "style": "IPY_MODEL_55e5f329c5a94f019e296c19ed5a93bc",
       "value": " 901/? [06:07&lt;00:00,  2.45it/s]"
      }
     },
     "c8e6ae23c37a4609b09de9a174995260": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b4da74adf29e4d89be098ff0a9aa26bf",
        "IPY_MODEL_c144991e20514a4abf9612e81a97645f"
       ],
       "layout": "IPY_MODEL_9f0f1b898ae94430a4932fe5e28d606b"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
